{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbcb3973",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "The purpose of this notebook is to generate figure 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436df63e",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afce0fc",
   "metadata": {},
   "source": [
    "## load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54ff771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%autosave 60\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#filesystem\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "#plotting\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from PIL import Image\n",
    "from dateutil import rrule\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "#data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import random\n",
    "import ast\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import community\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# custom modules\n",
    "from src import parameters,rfid, plot\n",
    "from src import timestamps\n",
    "from src import spectrogramming\n",
    "from src import preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790f86c",
   "metadata": {},
   "source": [
    "## set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ID of the DAS model that generated the segments\n",
    "DAS_model = '20230219_120047'\n",
    "\n",
    "# main project root - to reproduce, just change root to the full path to wild-mus-vocal-ecology\n",
    "root = '/path/to/wild-mus-vocal-ecology'\n",
    "\n",
    "# this directory contains jpegs\n",
    "pictures_root = os.path.join(root, 'parameters','images','barn_pictures')\n",
    "\n",
    "# this directory contains raw vocalization segments\n",
    "vocal_counts_path = os.path.join(root, 'data', 'segments', 'vocal_counts')\n",
    "\n",
    "# path to dictionary recording which boxes were recorded when and with which audiomoth\n",
    "boxes_dictionary = os.path.join(root, 'parameters', 'json_files', 'boxes_recorded.json')\n",
    "\n",
    "# paths to dictionaries for coloring plots\n",
    "vocalization_colors = os.path.join(root, 'parameters', 'json_files', 'vocalization_colors.json')\n",
    "seasons_colors = os.path.join(root, 'parameters', 'json_files','season_color_dict.json')\n",
    "audiomoth_colors = os.path.join(root, 'parameters', 'json_files','audiomoth_colors.json')\n",
    "\n",
    "# paths to rfid reading directories (each contains a directory for meets, stays, and box events)\n",
    "all_rfid = os.path.join(root, 'data', 'rfid') # path to rfid data from 2013-2023 (\"all\")\n",
    "\n",
    "# path to adult phenotypes (with sex)\n",
    "adult_sexes_path = os.path.join(root, 'data', 'phenotypes', 'sexes.csv')\n",
    "\n",
    "# timestamp (string) of umap embedding to plot in yyyymmdd_hhmmss format\n",
    "umap_ID = '20230707_111132'\n",
    "\n",
    "# paths to vocal events data\n",
    "vocal_events_path = os.path.join(root,'data', 'segments', 'vocal_events' )\n",
    "\n",
    "# paths to example spectrograms\n",
    "spectrograms_path = os.path.join(root,'data', 'umap', 'spectrograms' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09528c3a",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362abc6b",
   "metadata": {},
   "source": [
    "## Vocal events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b67132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vocal events - all recordings\n",
    "print('getting vocal events 2022-2023...')\n",
    "v_events = pd.concat([pd.read_csv(i) for i in glob.glob(os.path.join(vocal_events_path, '*segments.csv'))])\n",
    "v_events = preprocess.raw_data_from_cloud_of_mice(v_events, df_type='vocal_events')\n",
    "\n",
    "#vocal counts - all recordings\n",
    "print('getting vocal counts 2022-2023...')\n",
    "v_counts = pd.concat([pd.read_csv(i) for i in glob.glob(os.path.join(vocal_counts_path, '*counts.csv'))])\n",
    "v_counts = preprocess.raw_data_from_cloud_of_mice(v_counts, df_type='vocal_counts')\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804bd79",
   "metadata": {},
   "source": [
    "## Organize the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692c242",
   "metadata": {},
   "source": [
    "### Percent vocal events by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping v_events by deployment and getting the first occurrence of yearless_season for each deployment\n",
    "v_events['yearless_season'] = [i.split('_')[0] for i in v_events['season']]\n",
    "v_events = v_events[v_events['label'] != 'noise']\n",
    "v_events_by_deployment = v_events[['deployment', 'yearless_season']].groupby('deployment').agg({'yearless_season':'first'}).reset_index()\n",
    "\n",
    "# Counting the number of deployments for each season\n",
    "num_summer_deployments = len(v_events_by_deployment[v_events_by_deployment['yearless_season'] == 'summer'])\n",
    "num_winter_deployments = len(v_events_by_deployment[v_events_by_deployment['yearless_season'] == 'winter'])\n",
    "num_autumn_deployments = len(v_events_by_deployment[v_events_by_deployment['yearless_season'] == 'autumn'])\n",
    "num_spring_deployments = len(v_events_by_deployment[v_events_by_deployment['yearless_season'] == 'spring'])\n",
    "\n",
    "# Printing the counts for each season\n",
    "print(f'Number of summer deployments: {num_summer_deployments}')\n",
    "print(f'Number of winter deployments: {num_winter_deployments}')\n",
    "print(f'Number of autumn deployments: {num_autumn_deployments}')\n",
    "print(f'Number of spring deployments: {num_spring_deployments}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44caa832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#average detected vocalizations per box\n",
    "summer_vocs = v_events[v_events['yearless_season'] == 'summer']\n",
    "winter_vocs = v_events[v_events['yearless_season'] == 'winter']\n",
    "autumn_vocs = v_events[v_events['yearless_season'] == 'autumn']\n",
    "spring_vocs = v_events[v_events['yearless_season'] == 'spring']\n",
    "total_vocs = len(v_events)\n",
    "vocs_per_box = total_vocs/144\n",
    "minutes_per_box = ((6594.3)*60)/144\n",
    "vocs_per_minute_per_box = vocs_per_box/minutes_per_box\n",
    "num_squeaks = len(v_events[v_events['label'] == 'squeak'])\n",
    "num_USV = len(v_events[v_events['label'] == 'USV'])\n",
    "print('There are', total_vocs, 'vocalizations, correspinding to', vocs_per_minute_per_box, \"vocalizations per minute per box\")\n",
    "print(num_squeaks, 'of these are squeaks and', num_USV, \"are USVs\")\n",
    "print(f\"There are {num_squeaks/num_USV} times more squeaks than USVs\\n\\n\")\n",
    "print(len(summer_vocs)/total_vocs, \"of vocalizations took place in summer.\\n\")\n",
    "print(len(winter_vocs)/total_vocs, \"of vocalizations took place in winter.\\n\")\n",
    "print(len(autumn_vocs)/total_vocs, \"of vocalizations took place in autumn.\\n\")\n",
    "print(len(spring_vocs)/total_vocs, \"of vocalizations took place in spring.\\n\")\n",
    "print((len(spring_vocs)+len(summer_vocs))/total_vocs, \"of vocalizations took place in spring and summer.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b04ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocalization: 1366171\n",
      "Total squeaks: 1260452\n",
      "Total vocalization: 105719\n"
     ]
    }
   ],
   "source": [
    "# total vocalizations, squeaks, and USVs\n",
    "print(f\"Total vocalization: {len(v_events)}\")\n",
    "print(f\"Total squeaks: {len(v_events[v_events['label'] == 'squeak'])}\")\n",
    "print(f\"Total vocalization: {len(v_events[v_events['label'] == 'USV'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bda3b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vocalizations per box: 3.4529088252986164\n"
     ]
    }
   ],
   "source": [
    "#average detected vocalizations per box\n",
    "vocs_per_box = 1366171/144\n",
    "minutes_per_box = ((6594.3)*60)/144\n",
    "vocs_per_minute_per_box = vocs_per_box/minutes_per_box\n",
    "print('Average vocalizations per box:', vocs_per_minute_per_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663da178",
   "metadata": {},
   "source": [
    "### Get vocalization counts by time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c378a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seasons = []   \n",
    "for season in v_events['season'].unique():\n",
    "    \n",
    "    df = timestamps.get_vocalizations_by_hour_of_day(season = season, \n",
    "                                                     v_events = v_events)\n",
    "    df['season'] = season\n",
    "    all_seasons.append(df)\n",
    "    \n",
    "vocs_by_time_of_day = pd.concat(all_seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1248e",
   "metadata": {},
   "source": [
    "### Calculate squeak--USV co-occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09e0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = v_counts.copy()\n",
    "\n",
    "# filter \n",
    "filtered_df = df\n",
    "\n",
    "# get observed overlap\n",
    "observed_overlap = len(filtered_df[(filtered_df['squeak_count'] > 0) & (filtered_df['USV_count'] > 0)])/len(filtered_df)\n",
    "\n",
    "# simulate expected overlap\n",
    "n_permutations = 1000  # Number of permutations\n",
    "expected_overlaps = []\n",
    "\n",
    "for _ in tqdm(range(n_permutations)):\n",
    "    shuffled_squeak_count = np.random.permutation(filtered_df['squeak_count'])\n",
    "    shuffled_USV_count = np.random.permutation(filtered_df['USV_count'])\n",
    "    expected_overlap = len(filtered_df[(shuffled_squeak_count > 0) & (shuffled_USV_count > 0)])\n",
    "    expected_overlaps.append(expected_overlap/len(filtered_df))\n",
    "\n",
    "# compare\n",
    "expected_overlaps = np.array(expected_overlaps)\n",
    "\n",
    "p_value_right = (np.sum(expected_overlaps >= observed_overlap) + 1) / (n_permutations + 1)\n",
    "p_value_left = (np.sum(expected_overlaps <= observed_overlap) + 1) / (n_permutations + 1)\n",
    "p_value_two_tailed = 2 * min(p_value_right, p_value_left)\n",
    "\n",
    "print(f'Observed overlap: {observed_overlap}')\n",
    "print(f'Mean expected overlap: {np.mean(expected_overlaps)}')\n",
    "print(f'P-value (right tail): {p_value_right}')\n",
    "print(f'P-value (left tail): {p_value_left}')\n",
    "print(f'P-value (two-tailed): {p_value_two_tailed}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638d896",
   "metadata": {},
   "source": [
    "## Make the Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8930627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save or not\n",
    "save = False\n",
    "path_to_save = ''\n",
    "\n",
    "##################################################################################################\n",
    "root = '/path/to/wild-mus-vocal-ecology/'\n",
    "umap_df = pd.read_feather(os.path.join(root, 'data/umap/20230707_111132_UMAPembedding_withfeatures.feather'))\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "# plotting parameters\n",
    "plt.style.use('default')\n",
    "tick_label_fontsize = 6\n",
    "axis_label_fontsize = 9\n",
    "title_label_fontsize = 12\n",
    "num_ticks = 40 # for panel F x-axis\n",
    "dot_size = .5\n",
    "dot_alpha = .5\n",
    "voc_colormap = parameters.load_json(vocalization_colors)\n",
    "season_colormap = parameters.load_json(seasons_colors)\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(mosaic=\"EEEEIIJJ.;\"\\\n",
    "                                      \"EEEELLMMk;\"\\\n",
    "                                      \"EEEELLMMk;\"\\\n",
    "                                      \"FFFFLLMMk;\"\\\n",
    "                                      \"FFFFLLMMk;\"\n",
    "                                      \"FFFFLLMMk;\"\\\n",
    "                                      \"FFFFLLMMk;\"\\\n",
    "                                      \"OOOORRRR.;\"\\\n",
    "                                      \"OOOORRRR.;\"\\\n",
    "                                      \"PPPPQQQQ.;\"\\\n",
    "                                      \"PPPPQQQQ.\",\n",
    "\n",
    "                               figsize=[8.5,11], \n",
    "                               gridspec_kw={'height_ratios': [0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5, 0.5, 0.5], \n",
    "                                            'width_ratios':[1,1,1,1,1,1, 1, 1, 0.1]},\n",
    "                               tight_layout=True,\n",
    "                               dpi=600)\n",
    "\n",
    "##################################################################################################\n",
    "### PANELS E - UMAP embedding\n",
    "##################################################################################################\n",
    "embedding = umap_df\n",
    "sns.scatterplot(data = embedding,\n",
    "                x = 'umap1',\n",
    "                y = 'umap2',\n",
    "                hue = embedding['label'],\n",
    "                palette = voc_colormap,\n",
    "                s = dot_size,\n",
    "                alpha = dot_alpha, \n",
    "                ax = axes[\"E\"], \n",
    "                linewidth = 0,\n",
    "                legend = 'brief')\n",
    "\n",
    "axes[\"E\"].set_xlim([-2.5,12.5])\n",
    "axes[\"E\"].set_ylim([-2.5,12.5])\n",
    "axes[\"E\"].legend(frameon=False)\n",
    "axes[\"E\"].axis('off')\n",
    "\n",
    "#################################################################################################\n",
    "## PANELS F - example spectrograms - NB this requires access to raw audio\n",
    "#################################################################################################\n",
    "\n",
    "num_freq_bins = 1024\n",
    "num_time_bins = 1024\n",
    "spec_min_val = 1.5\n",
    "spec_max_val = 4.5\n",
    "nperseg = 512\n",
    "fs = 192000\n",
    "\n",
    "squeak_spec_params = {\n",
    "    'fs':fs,\n",
    "    'nperseg':nperseg,\n",
    "    'noverlap':nperseg//4,\n",
    "    'num_freq_bins':num_freq_bins,\n",
    "    'num_time_bins':num_time_bins,\n",
    "    'min_freq':0,\n",
    "    'max_freq':96000,\n",
    "    'fill_value':spec_min_val,\n",
    "    'max_duration':0.25,\n",
    "    'spec_min_val':spec_min_val,\n",
    "    'spec_max_val':spec_max_val\n",
    "}\n",
    "\n",
    "USV_spec_params = {\n",
    "    'fs':fs,\n",
    "    'nperseg':nperseg,\n",
    "    'noverlap':nperseg//4,\n",
    "    'num_freq_bins':num_freq_bins,\n",
    "    'num_time_bins':num_time_bins,\n",
    "    'min_freq':0,\n",
    "    'max_freq':96000,\n",
    "    'fill_value':spec_min_val,\n",
    "    'max_duration':0.06,\n",
    "    'spec_min_val':spec_min_val,\n",
    "    'spec_max_val':spec_max_val\n",
    "}\n",
    "\n",
    "# Create a grid of square images within subplot F\n",
    "axes[\"F\"].axis('off')\n",
    "num_rows, num_cols = 4, 4\n",
    "image_size = 0.2 \n",
    "inset_axes_dict = {}\n",
    "inset_ID = 1\n",
    "spacing = 0.02\n",
    "\n",
    "\n",
    "for row in range(num_rows):\n",
    "    for col in range(num_cols):\n",
    "        x_pos = (col / num_cols) + 0.05\n",
    "        y_pos = 1 - (row + 1) / num_rows\n",
    "        inset_axes_dict[inset_ID] = axes['F'].inset_axes([x_pos, y_pos, image_size, image_size+spacing])\n",
    "        inset_ID+=1\n",
    "\n",
    "#the example USVs \n",
    "USV_names = ['audiomoth02_20220917_031100_clip1',\n",
    "            'audiomoth03_20220903_094400_clip10',\n",
    "            'audiomoth03_20220818_041700_clip5',\n",
    "            'audiomoth03_20230107_083200_clip27',\n",
    "            'audiomoth03_20220819_091300_clip11',\n",
    "            'audiomoth03_20220903_094300_clip10',\n",
    "            'audiomoth03_20220904_065600_clip4',\n",
    "            'audiomoth03_20230107_192400_clip1']\n",
    "USV_to_plot = embedding[embedding['clip_name'].isin(USV_names)]\n",
    "\n",
    "# plot the spectrograms and decorate the UMAP\n",
    "USV_txt = [1,2,5,6,9,10,13,14]\n",
    "for USV_txt, umap1, umap2 in zip(USV_txt,USV_to_plot['umap1'], USV_to_plot['umap2']):\n",
    "    voc = USV_to_plot[(USV_to_plot['umap1'] == umap1) & (USV_to_plot['umap2'] == umap2)] # one vocalization\n",
    "    axes[\"E\"].scatter(voc['umap1'],voc['umap2'],c = 'black',s = dot_size+5, linewidth=0,alpha = dot_alpha) #umap dot\n",
    "    axes[\"E\"].annotate(text=USV_txt, xy=(umap1,  umap2), xytext=(0.5,0.5), textcoords = 'offset points', color='black', fontsize=tick_label_fontsize) #umap dot number\n",
    "    path_to_spec = os.path.join(root, 'data', 'umap', 'spectrograms', voc['clip_name'].iloc[0]+\".npy\")\n",
    "    spec = np.load(path_to_spec)\n",
    "    inset_axes_dict[int(USV_txt)].imshow(spec, origin='lower', extent = (num_freq_bins, 0, num_time_bins, 0 ), aspect='equal')\n",
    "    inset_axes_dict[int(USV_txt)].text(118, 10, USV_txt, ha=\"center\", va=\"top\", color=\"w\", fontsize=axis_label_fontsize, fontname='Arial')\n",
    "    inset_axes_dict[int(USV_txt)].axis('off')\n",
    "\n",
    "#the example squeaks\n",
    "squeak_names = ['audiomoth01_20220818_074400_clip54',\n",
    "                 'audiomoth01_20230317_035600_clip2',\n",
    "                 'audiomoth02_20230311_165300_clip18',\n",
    "                 'audiomoth01_20230317_035800_clip54',\n",
    "                 'audiomoth01_20221126_183000_clip13',\n",
    "                 'audiomoth01_20221015_165000_clip3',\n",
    "                 'audiomoth01_20220818_064400_clip4',\n",
    "                 'audiomoth01_20220818_012000_clip28']\n",
    "squeaks_to_plot = embedding[embedding['clip_name'].isin(squeak_names)]  \n",
    "\n",
    "# plot the spectrograms and decorate the UMAP\n",
    "squeaks_txt = [3,4,7,8,11,12,15,16]\n",
    "for squeaks_txt, umap1, umap2 in zip(squeaks_txt,squeaks_to_plot['umap1'], squeaks_to_plot['umap2']):\n",
    "    voc = squeaks_to_plot[(squeaks_to_plot['umap1'] == umap1) & (squeaks_to_plot['umap2'] == umap2)] # one vocalization\n",
    "    axes[\"E\"].scatter(voc['umap1'],voc['umap2'],c = 'black',s = dot_size+5, linewidth=0,alpha = dot_alpha) #umap dot\n",
    "    axes[\"E\"].annotate(text=squeaks_txt, xy=(umap1,  umap2), xytext=(0.5,0.5), textcoords = 'offset points', color='black', fontsize=tick_label_fontsize) #umap dot number\n",
    "    path_to_spec = os.path.join(root, 'data', 'umap', 'spectrograms', voc['clip_name'].iloc[0]+\".npy\")\n",
    "    spec = np.load(path_to_spec)\n",
    "    inset_axes_dict[int(squeaks_txt)].imshow(spec, origin='lower', extent = (num_freq_bins, 0, num_time_bins, 0 ), aspect='equal')\n",
    "    inset_axes_dict[int(squeaks_txt)].text(118, 10, squeaks_txt, ha=\"center\", va=\"top\", color=\"w\", fontsize=axis_label_fontsize, fontname='Arial')\n",
    "    inset_axes_dict[int(squeaks_txt)].axis('off')\n",
    "\n",
    "##################################################################################################\n",
    "### PANEL & - vocal counts by time of day\n",
    "##################################################################################################\n",
    "\n",
    "season_colormap = parameters.load_json(seasons_colors)\n",
    "season_order = ['summer_22', 'autumn_22', 'winter', 'spring', 'summer_23', 'autumn_23']\n",
    "\n",
    "# Pivot the DataFrame to get hours as columns and dates as rows\n",
    "USV_heatmap_data = vocs_by_time_of_day.pivot_table(\n",
    "    index=['season', 'deployment', 'moth'],  # Rows represent each recorded day\n",
    "    columns=['hour'],  # Columns represent each hour interval\n",
    "    values='USV_count',  # Values to fill the heatmap\n",
    "    fill_value=0  # Fill missing values with 0\n",
    ")\n",
    "# Convert 'season' index level to a categorical type with the defined order\n",
    "USV_heatmap_data.index = pd.MultiIndex.from_frame(\n",
    "    USV_heatmap_data.index.to_frame().assign(\n",
    "        season=pd.Categorical(USV_heatmap_data.index.get_level_values('season'), categories=season_order, ordered=True)\n",
    "    ),\n",
    "    names=['season', 'deployment', 'moth']\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the new categorical index\n",
    "USV_heatmap_data = USV_heatmap_data.sort_index()\n",
    "\n",
    "# Define the desired order for seasons\n",
    "\n",
    "# Pivot the DataFrame to get hours as columns and dates as rows\n",
    "squeak_heatmap_data = vocs_by_time_of_day.pivot_table(\n",
    "    index=['season', 'deployment', 'moth'],  # Rows represent each recorded day\n",
    "    columns=['hour'],  # Columns represent each hour interval\n",
    "    values='squeak_count',  # Values to fill the heatmap\n",
    "    fill_value=0  # Fill missing values with 0\n",
    ")\n",
    "squeak_heatmap_data.index = pd.MultiIndex.from_frame(squeak_heatmap_data.index.to_frame().assign(\n",
    "        season=pd.Categorical(squeak_heatmap_data.index.get_level_values('season'), categories=season_order, ordered=True)\n",
    "    ),\n",
    "    names=['season', 'deployment', 'moth']\n",
    ")\n",
    "squeak_heatmap_data = squeak_heatmap_data.sort_index()\n",
    "\n",
    "# Normalize the rows to the maximum value in each row\n",
    "USV_heatmap_data_normalized = USV_heatmap_data.div(USV_heatmap_data.max(axis=1), axis=0).fillna(0)\n",
    "squeak_heatmap_data_normalized = squeak_heatmap_data.div(squeak_heatmap_data.max(axis=1), axis=0).fillna(0)\n",
    "\n",
    "# Aggregate data for line plots\n",
    "vocs_by_time_of_day['yearless_season'] = [i.split('_')[0] for i in vocs_by_time_of_day['season'] ]\n",
    "lineplot_data = vocs_by_time_of_day.groupby(['hour', 'yearless_season'])[['USV_count', 'squeak_count']].sum().reset_index()\n",
    "lineplot_data['USV_count'] = lineplot_data.groupby('yearless_season')['USV_count'].transform(lambda x: x / x.max())\n",
    "lineplot_data['squeak_count'] = lineplot_data.groupby('yearless_season')['squeak_count'].transform(lambda x: x / x.max())\n",
    "\n",
    "# Plot the lineplots using seaborn\n",
    "sns.lineplot(data=lineplot_data, \n",
    "             x='hour', \n",
    "             y='USV_count', \n",
    "             hue='yearless_season', \n",
    "             palette=season_colormap, \n",
    "             ax=axes[\"J\"], legend = False)\n",
    "sns.lineplot(data=lineplot_data, \n",
    "             x='hour', \n",
    "             y='squeak_count', \n",
    "             hue='yearless_season', \n",
    "             palette=season_colormap, \n",
    "             ax=axes[\"I\"], legend = False)\n",
    "\n",
    "# Remove x-axis labels for lineplots\n",
    "axes[\"J\"].set_xticklabels([])\n",
    "axes[\"I\"].set_xticklabels([])\n",
    "axes[\"J\"].tick_params(axis = 'y', labelsize = tick_label_fontsize)\n",
    "axes[\"I\"].tick_params(axis = 'y', labelsize = tick_label_fontsize)\n",
    "\n",
    "# Set y-axis labels for lineplots\n",
    "axes[\"J\"].set_ylabel('')\n",
    "axes[\"I\"].set_ylabel('')\n",
    "axes[\"J\"].set_xlabel('')\n",
    "axes[\"I\"].set_xlabel('')\n",
    "\n",
    "# Plot the heatmaps\n",
    "sns.heatmap(USV_heatmap_data_normalized, cmap='YlGnBu', annot=False, cbar=False, ax=axes[\"M\"])\n",
    "axes[\"M\"].set_title('')\n",
    "axes[\"M\"].set_xlabel('Hour', fontsize = axis_label_fontsize)\n",
    "axes[\"M\"].set_ylabel('')\n",
    "axes[\"M\"].set_yticklabels([])\n",
    "axes[\"M\"].tick_params(axis = 'x', labelsize = tick_label_fontsize)\n",
    "\n",
    "# Plot the heatmaps\n",
    "sns.heatmap(squeak_heatmap_data_normalized, cmap='YlGnBu', annot=False, cbar=False, ax=axes[\"L\"])\n",
    "axes[\"L\"].set_title('')\n",
    "axes[\"L\"].set_xlabel('Hour', fontsize = axis_label_fontsize)\n",
    "axes[\"L\"].set_ylabel('Recorded Date', fontsize = axis_label_fontsize)\n",
    "axes[\"L\"].set_yticklabels([])\n",
    "axes[\"L\"].tick_params(axis = 'x', labelsize = tick_label_fontsize)\n",
    "axes[\"L\"].tick_params(axis = 'y', labelsize = tick_label_fontsize)\n",
    "\n",
    "# Convert the season names to numeric codes\n",
    "season_annotation_df = pd.DataFrame(columns = ['season'])\n",
    "season_annotation_df['season'] = squeak_heatmap_data.reset_index()['season']\n",
    "season_annotation_df_numeric = season_annotation_df.applymap(lambda x: list(season_colormap.keys()).index(x))\n",
    "\n",
    "# Create a color list from the colormap\n",
    "colors = list(season_colormap.values())\n",
    "cmap = mcolors.ListedColormap(colors)\n",
    "bounds = list(range(len(season_colormap) + 1))\n",
    "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(\n",
    "    season_annotation_df_numeric,\n",
    "    cmap=cmap,\n",
    "    annot=False,\n",
    "    cbar=False,\n",
    "    ax=axes['k'],\n",
    "    linewidths=0.5,\n",
    "    norm=norm\n",
    ")\n",
    "\n",
    "axes['k'].set_title('')\n",
    "axes['k'].set_xlabel('')\n",
    "axes['k'].set_ylabel('')\n",
    "for ax in [axes['k']]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "sns.despine(ax = axes[\"J\"])\n",
    "sns.despine(ax = axes[\"I\"])\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "### PANELS O and P - minutes with vocalization by deployment\n",
    "##################################################################################################\n",
    "\n",
    "#convert the \"deployment\" column to a categorical data type for regression\n",
    "v_counts['deployment'] = pd.Categorical(v_counts['deployment'], ordered=True)\n",
    "v_counts['deployment_codes'] = v_counts['deployment'].cat.codes\n",
    "\n",
    "#calculate the percentage of minutes with > 1 vocalization counts for each value of \"deployment\"\n",
    "contains_squeaks = v_counts.groupby(['season', 'deployment_codes', 'moth'])['squeak_count'].apply(lambda x: (x > 0).mean() * 100).reset_index().rename(columns = {'squeak_count':'percent_minutes_with_squeaks'})\n",
    "contains_USVs = v_counts.groupby(['season','deployment_codes', 'moth'])['USV_count'].apply(lambda x: (x > 0).mean() * 100).reset_index().rename(columns = {'USV_count':'percent_minutes_with_USV'})\n",
    "percent_minutes_with_vocs = contains_USVs.merge(contains_squeaks, on = ['season', 'deployment_codes', 'moth'])\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(data=contains_squeaks, \n",
    "            x='deployment_codes', \n",
    "            y='percent_minutes_with_squeaks', \n",
    "            s=15,\n",
    "            c = contains_squeaks['season'].map(season_colormap),\n",
    "            linewidth=0,\n",
    "            ax=axes[\"O\"])\n",
    "sns.scatterplot(data=contains_USVs, \n",
    "            x='deployment_codes', \n",
    "            y='percent_minutes_with_USV', \n",
    "            s=15,\n",
    "            c = contains_USVs['season'].map(season_colormap),\n",
    "            linewidth=0,\n",
    "            ax=axes[\"P\"])\n",
    "\n",
    "mean_squeaks = contains_squeaks.groupby('deployment_codes')['percent_minutes_with_squeaks'].median().reset_index()\n",
    "axes[\"O\"].plot(mean_squeaks['deployment_codes'], \n",
    "               mean_squeaks['percent_minutes_with_squeaks'], \n",
    "               color='grey', \n",
    "               linestyle='-', \n",
    "               linewidth=3, \n",
    "               alpha = 0.5,\n",
    "               marker='o', \n",
    "               markersize = 0)\n",
    "\n",
    "mean_USVs = contains_USVs.groupby('deployment_codes')['percent_minutes_with_USV'].median().reset_index()\n",
    "axes[\"P\"].plot(mean_USVs['deployment_codes'], \n",
    "               mean_USVs['percent_minutes_with_USV'], \n",
    "               color='grey', \n",
    "               linestyle='-', \n",
    "               alpha = 0.5,\n",
    "               linewidth=3, \n",
    "               marker='o', \n",
    "               markersize = 0)\n",
    "\n",
    "# modify plots    \n",
    "deployment_dates = [i[4:] for i in sorted([i.split('-')[0] for i in v_counts['deployment'].unique()])]\n",
    "for ax in [\"O\", \"P\"]:\n",
    "    axes[ax].set_xticks(range(len(deployment_dates)))\n",
    "    axes[ax].set_xticklabels(deployment_dates, rotation=90, fontsize=6)\n",
    "    axes[ax].tick_params(axis='x', length=0)\n",
    "    axes[ax].tick_params(axis='y', labelsize=6)\n",
    "    sns.despine(ax=axes[ax])\n",
    "\n",
    "    if ax == \"O\":\n",
    "        axes[ax].set_title('Squeaks', fontsize=axis_label_fontsize)\n",
    "        axes[ax].set_xlabel('')\n",
    "        axes[ax].set_ylabel('Minutes with vocalization (%)', fontsize=axis_label_fontsize)\n",
    "    elif ax == \"P\":\n",
    "        axes[ax].set_title('USVs', fontsize=9)\n",
    "        axes[ax].set_xlabel('recording start (mmdd)', fontsize=axis_label_fontsize)\n",
    "        axes[ax].set_ylabel('', fontsize=6)\n",
    "\n",
    "##################################################################################################\n",
    "### PANEL R minutes with squeak vs USV \n",
    "##################################################################################################\n",
    "df = percent_minutes_with_vocs.copy()\n",
    "df['yearless_season'] = [i.split('-')[0] for i in df['season']]\n",
    "\n",
    "correlation, p_value = pearsonr(df['percent_minutes_with_squeaks'], df['percent_minutes_with_USV'])\n",
    "\n",
    "sns.scatterplot(data = df, \n",
    "                x = 'percent_minutes_with_squeaks', \n",
    "                y = 'percent_minutes_with_USV', \n",
    "                ax = axes[\"R\"], \n",
    "                hue = 'yearless_season', \n",
    "                palette = season_colormap,\n",
    "                s = 15, alpha = 0.75,\n",
    "                legend = False)\n",
    "sns.regplot(data=df, \n",
    "            x='percent_minutes_with_squeaks', \n",
    "            y='percent_minutes_with_USV', \n",
    "            scatter=False, \n",
    "            color='grey', \n",
    "            line_kws={\"linestyle\":\"--\", \"linewidth\":2}, \n",
    "            ax = axes[\"R\"])\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "axes[\"R\"].tick_params(axis = 'both', labelsize = tick_label_fontsize)\n",
    "axes[\"R\"].set_xlabel('% minutes with squeaks', fontsize = axis_label_fontsize)\n",
    "axes[\"R\"].set_ylabel('% minutes with USVs', fontsize = axis_label_fontsize)\n",
    "sns.despine()\n",
    "\n",
    "##################################################################################################\n",
    "### PANEL Q squeak vs USV co-occurance\n",
    "##################################################################################################\n",
    "\n",
    "sns.histplot(expected_overlaps, \n",
    "             kde=False, \n",
    "             bins=10, \n",
    "             ax = axes[\"Q\"], \n",
    "             linewidth = 0, \n",
    "             color = 'grey')\n",
    "axes[\"Q\"].axvline(observed_overlap, \n",
    "                  color='red', \n",
    "                  linestyle='dashed', \n",
    "                  linewidth=1)\n",
    "axes[\"Q\"].set_xlabel('Minutes with squeaks and USVs co-occurring (% total)', fontsize = axis_label_fontsize)\n",
    "axes[\"Q\"].set_ylabel('Frequency', fontsize = axis_label_fontsize)\n",
    "axes[\"Q\"].text(observed_overlap + axes[\"Q\"].get_xlim()[1] * 0.005, \n",
    "               axes[\"Q\"].get_ylim()[1] * 0.9, \n",
    "               'Actual Overlap\\n(p < 0.001)', \n",
    "               fontsize=tick_label_fontsize, \n",
    "               color='red', \n",
    "               ha='left')\n",
    "\n",
    "axes[\"Q\"].text(np.mean(expected_overlaps) + axes[\"Q\"].get_xlim()[1] * 0.005, \n",
    "               axes[\"Q\"].get_ylim()[1] * 0.9, \n",
    "               'Expected Overlaps\\n(1,000 permutations)', \n",
    "               fontsize=tick_label_fontsize, \n",
    "               color='grey', \n",
    "               ha='left')\n",
    "\n",
    "axes[\"Q\"].tick_params(axis = 'both', labelsize = tick_label_fontsize)\n",
    "axes[\"Q\"].set_xlim([0,0.03])\n",
    "sns.despine()\n",
    "\n",
    "#save\n",
    "if save:\n",
    "    plt.savefig(os.path.join(path_to_save, 'Figure4.svg'))\n",
    "    plt.savefig(os.path.join(path_to_save, 'Figure4.jpeg'), dpi = 600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3163a6f1",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b62681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.multicomp as multi\n",
    "import statsmodels.stats.multitest as smm\n",
    "from statsmodels.formula.api import ols\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from src import modelselection\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e91dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_dir = ''\n",
    "save = False # if true, save statistics csvs and output to resubmission_statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f59a1",
   "metadata": {},
   "source": [
    "### Panel C\n",
    "minutes with squeaks ~ season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42be994",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = percent_minutes_with_vocs\n",
    "dependent_var = 'percent_minutes_with_squeaks'\n",
    "independent_var = 'yearless_season'\n",
    "data['yearless_season'] = [i.split('_')[0] for i in data['season']]\n",
    "independent_var = 'yearless_season'\n",
    "data = data.dropna(subset = dependent_var)\n",
    "analysis_name = 'figure4_panelC'\n",
    "\n",
    "order = ['summer', 'autumn', 'winter', 'spring']\n",
    "sns.boxplot(data = data, \n",
    "            x = 'yearless_season', \n",
    "            y = dependent_var, \n",
    "            order = order)\n",
    "\n",
    "# Fit the one-way ANOVA model\n",
    "formula = dependent_var + f\" ~ C({independent_var})\"\n",
    "model = ols(formula, data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "print(anova_table)\n",
    "\n",
    "# Perform Tukey's HSD post-hoc test\n",
    "mc = multi.MultiComparison(data[dependent_var], data[independent_var])\n",
    "tukey_result = mc.tukeyhsd()\n",
    "tukey_result = pd.DataFrame(data=tukey_result.summary().data[1:], columns=tukey_result.summary().data[0])\n",
    "\n",
    "# Print the results of the Tukey HSD test\n",
    "print(tukey_result)\n",
    "\n",
    "if save:\n",
    "    \n",
    "    #save the analysis results\n",
    "    modelselection.save_ANOVA_tables_to_word(analysis_name+': '+formula, anova_table, tukey_result, os.path.join(statistics_dir, analysis_name +'.docx'))\n",
    "    \n",
    "    #save the data the analysis was run on\n",
    "    data.to_csv(os.path.join(statistics_dir, analysis_name + '.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f80c37b",
   "metadata": {},
   "source": [
    "### Panel D\n",
    "Percent minutes with USV ~ season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e3ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = percent_minutes_with_vocs\n",
    "dependent_var = 'percent_minutes_with_USV'\n",
    "independent_var = 'yearless_season'\n",
    "\n",
    "data['yearless_season'] = [i.split('_')[0] for i in data['season']]\n",
    "independent_var = 'yearless_season'\n",
    "data = data.dropna(subset = dependent_var)\n",
    "analysis_name = 'figure4_panelD'\n",
    "\n",
    "order = ['summer', 'autumn', 'winter', 'spring']\n",
    "sns.boxplot(data = data, \n",
    "            x = 'yearless_season', \n",
    "            y = dependent_var, \n",
    "            order = order)\n",
    "\n",
    "# Fit the one-way ANOVA model\n",
    "formula = dependent_var + f\" ~ C({independent_var})\"\n",
    "model = ols(formula, data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "print(anova_table)\n",
    "\n",
    "# Perform Tukey's HSD post-hoc test\n",
    "mc = multi.MultiComparison(data[dependent_var], data[independent_var])\n",
    "tukey_result = mc.tukeyhsd()\n",
    "tukey_result = pd.DataFrame(data=tukey_result.summary().data[1:], columns=tukey_result.summary().data[0])\n",
    "\n",
    "# Print the results of the Tukey HSD test\n",
    "print(tukey_result)\n",
    "\n",
    "if save:\n",
    "    \n",
    "    #save the analysis results\n",
    "    modelselection.save_ANOVA_tables_to_word(analysis_name+': '+formula, anova_table, tukey_result, os.path.join(statistics_dir, analysis_name +'.docx'))\n",
    "    \n",
    "    #save the data the analysis was run on\n",
    "    data.to_csv(os.path.join(statistics_dir, analysis_name + '.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757bf31",
   "metadata": {},
   "source": [
    "### Panel E\n",
    "vocalizations ~ (time of day) * (season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c6d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeaks\n",
    "# Define the bins and labels \n",
    "# 1 = early morning (midnight to 6am)\n",
    "# 2 = morning (6am to noon)\n",
    "# 3 = afternoon (noon to 6pm)\n",
    "# 4 = night\n",
    "bins = [0, 6, 12, 18, 24]\n",
    "labels = [1, 2, 3, 4]\n",
    "\n",
    "# Use pd.cut to create the hour_interval column\n",
    "vocs_by_time_of_day['hour_interval'] = pd.cut(vocs_by_time_of_day['hour'], bins=bins, labels=labels, right=True)\n",
    "vocs_by_time_of_day['yearless_season'] = [i.split('_')[0] for i in vocs_by_time_of_day['season']]\n",
    "dependent_var = 'squeak_count'\n",
    "data = vocs_by_time_of_day.dropna(subset = [dependent_var])\n",
    "\n",
    "\n",
    "for season in sorted(data['yearless_season'].unique()):\n",
    "    print(season)\n",
    "    analysis_name = 'figure4_panelE_squeak'+'_'+season\n",
    "    this_season  = data[data['yearless_season'] == season] \n",
    "    formula = dependent_var + ' ~ C(hour_interval)'\n",
    "    model = ols(formula, data=this_season).fit()\n",
    "    anova_table = sm.stats.anova_lm(model)\n",
    "    print(anova_table)\n",
    "    p_value = anova_table['PR(>F)']['C(hour_interval)']\n",
    "    \n",
    "    if p_value < (0.05)/4:\n",
    "        \n",
    "        # Combine season and hour into a single factor\n",
    "        this_season['group'] = this_season['yearless_season'].astype(str) + '-' + this_season['hour_interval'].astype(str)\n",
    "\n",
    "        # Perform Tukey's HSD post-hoc test\n",
    "        mc = multi.MultiComparison(this_season[dependent_var], this_season['group'])\n",
    "        tukey_result = mc.tukeyhsd()\n",
    "        tukey_df = pd.DataFrame(data=tukey_result.summary().data[1:], columns=tukey_result.summary().data[0])\n",
    "\n",
    "        # Filter to just show comparisons within hours\n",
    "        def is_same_hour(row):\n",
    "            group1_hour = row['group1'].split('-')[1]\n",
    "            group2_hour = row['group2'].split('-')[1]\n",
    "            return group1_hour == group2_hour\n",
    "\n",
    "        # Filter to just show comparisons within seasons\n",
    "        def is_same_season(row):\n",
    "            group1_season = row['group1'].split('-')[0]\n",
    "            group2_season = row['group2'].split('-')[0]\n",
    "            return group1_season == group2_season\n",
    "\n",
    "        tukey_result = tukey_df[tukey_df.apply(is_same_season, axis=1)]\n",
    "        print(tukey_result)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        tukey_result = pd.DataFrame()\n",
    "        print('not significant\\n')\n",
    "\n",
    "\n",
    "    if save:\n",
    "\n",
    "        #save the analysis results\n",
    "        modelselection.save_ANOVA_tables_to_word(analysis_name+': '+formula, anova_table, tukey_result, os.path.join(statistics_dir, analysis_name +'.docx'))\n",
    "\n",
    "        #save the data the analysis was run on\n",
    "        data.to_csv(os.path.join(statistics_dir, analysis_name + '.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71796d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USV\n",
    "# Define the bins and labels \n",
    "# 1 = early morning (midnight to 6am)\n",
    "# 2 = morning (6am to noon)\n",
    "# 3 = afternoon (noon to 6pm)\n",
    "# 4 = night (6pm to midnight)\n",
    "bins = [0, 6, 12, 18, 24]\n",
    "labels = [1, 2, 3, 4]\n",
    "\n",
    "# Use pd.cut to create the hour_interval column\n",
    "vocs_by_time_of_day['hour_interval'] = pd.cut(vocs_by_time_of_day['hour'], bins=bins, labels=labels, right=True)\n",
    "vocs_by_time_of_day['yearless_season'] = [i.split('_')[0] for i in vocs_by_time_of_day['season']]\n",
    "dependent_var = 'USV_count'\n",
    "data = vocs_by_time_of_day.dropna(subset = [dependent_var])\n",
    "analysis_name = 'figure4_panelE_USV'\n",
    "\n",
    "for season in sorted(data['yearless_season'].unique()):\n",
    "    print(season)\n",
    "    analysis_name = 'figure4_panelE_USV'+'_'+season\n",
    "    this_season  = data[data['yearless_season'] == season] \n",
    "    formula = dependent_var + ' ~ C(hour_interval)*C(hour_interval)'\n",
    "    model = ols(formula, data=this_season).fit()\n",
    "    anova_table = sm.stats.anova_lm(model)\n",
    "    print(anova_table)\n",
    "    p_value = anova_table['PR(>F)']['C(hour_interval)']\n",
    "    \n",
    "    if p_value < (0.05)/4: # bonferroni correction because were doing an ANOVA for each season\n",
    "        \n",
    "        # Combine season and hour into a single factor\n",
    "        this_season['group'] = this_season['yearless_season'].astype(str) + '-' + this_season['hour_interval'].astype(str)\n",
    "\n",
    "        # Perform Tukey's HSD post-hoc test\n",
    "        mc = multi.MultiComparison(this_season[dependent_var], this_season['group'])\n",
    "        tukey_result = mc.tukeyhsd()\n",
    "        tukey_df = pd.DataFrame(data=tukey_result.summary().data[1:], columns=tukey_result.summary().data[0])\n",
    "\n",
    "        # Filter to just show comparisons within hours\n",
    "        def is_same_hour(row):\n",
    "            group1_hour = row['group1'].split('-')[1]\n",
    "            group2_hour = row['group2'].split('-')[1]\n",
    "            return group1_hour == group2_hour\n",
    "\n",
    "        # Filter to just show comparisons within seasons\n",
    "        def is_same_season(row):\n",
    "            group1_season = row['group1'].split('-')[0]\n",
    "            group2_season = row['group2'].split('-')[0]\n",
    "            return group1_season == group2_season\n",
    "\n",
    "        tukey_result = tukey_df[tukey_df.apply(is_same_season, axis=1)]\n",
    "        print(anova_table)\n",
    "        print(tukey_result)\n",
    "        \n",
    "        sns.boxplot(data = this_season, x = 'hour_interval', y = 'USV_count')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        tukey_result = pd.DataFrame()\n",
    "        print('not significant')\n",
    "\n",
    "    if save:\n",
    "\n",
    "        #save the analysis results\n",
    "        modelselection.save_ANOVA_tables_to_word(analysis_name+': '+formula, anova_table, tukey_result, os.path.join(statistics_dir, analysis_name +'.docx'))\n",
    "\n",
    "        #save the data the analysis was run on\n",
    "        data.to_csv(os.path.join(statistics_dir, analysis_name + '.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad99eb",
   "metadata": {},
   "source": [
    "### Panel F\n",
    "Correlation between squaks and USVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1842935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "data = percent_minutes_with_vocs\n",
    "\n",
    "# Calculate Pearson's correlation coefficient and p-value\n",
    "correlation, p_value = pearsonr(data['percent_minutes_with_squeaks'], data['percent_minutes_with_USV'])\n",
    "\n",
    "print(f'Pearson correlation coefficient: {correlation}')\n",
    "print(f'R^2: {correlation**2}')\n",
    "print(f'P-value: {p_value}')\n",
    "\n",
    "sns.scatterplot(data = data, x = 'percent_minutes_with_squeaks', y = 'percent_minutes_with_USV')\n",
    "sns.regplot(data=data, x='percent_minutes_with_squeaks', y='percent_minutes_with_USV', scatter=False, color='grey', line_kws={\"linestyle\":\"--\"})\n",
    "sns.despine()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c4166",
   "metadata": {},
   "source": [
    "### Panel G\n",
    "Squeak and USV co-occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f3c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0\n",
    "df = v_counts\n",
    "\n",
    "# Step 1: Filter the DataFrame\n",
    "#filtered_df = df[(df['squeak_count'] > 0) | (df['USV_count'] > 0)]\n",
    "filtered_df = df\n",
    "\n",
    "# Step 2: Calculate Observed Overlap\n",
    "observed_overlap = len(filtered_df[(filtered_df['squeak_count'] > 0) & (filtered_df['USV_count'] > 0)])/len(filtered_df)\n",
    "\n",
    "# Step 3: Simulate Expected Overlap\n",
    "n_permutations = 1000  # Number of permutations\n",
    "expected_overlaps = []\n",
    "\n",
    "for _ in tqdm(range(n_permutations)):\n",
    "    shuffled_squeak_count = np.random.permutation(filtered_df['squeak_count'])\n",
    "    shuffled_USV_count = np.random.permutation(filtered_df['USV_count'])\n",
    "    expected_overlap = len(filtered_df[(shuffled_squeak_count > 0) & (shuffled_USV_count > 0)])\n",
    "    expected_overlaps.append(expected_overlap/len(filtered_df))\n",
    "\n",
    "# Step 4: Compare Observed to Expected\n",
    "expected_overlaps = np.array(expected_overlaps)\n",
    "mean_expected_overlap = np.mean(expected_overlaps)\n",
    "std_expected_overlap = np.std(expected_overlaps)\n",
    "\n",
    "p_value_right = (np.sum(expected_overlaps >= observed_overlap) + 1) / (n_permutations + 1)\n",
    "p_value_left = (np.sum(expected_overlaps <= observed_overlap) + 1) / (n_permutations + 1)\n",
    "p_value_two_tailed = 2 * min(p_value_right, p_value_left)\n",
    "t_stat = (observed_overlap - np.mean(expected_overlaps)) / std_expected_overlap\n",
    "\n",
    "sns.histplot(expected_overlaps, kde=False, bins=30)\n",
    "plt.axvline(observed_overlap, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.xlabel('Overlapping Minutes (% Expected)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Expected Overlaps')\n",
    "plt.show()\n",
    "\n",
    "print(f'Observed overlap: {observed_overlap}')\n",
    "print(f'Mean expected overlap: {np.mean(expected_overlaps)}')\n",
    "print(f'P-value (right tail): {p_value_right}')\n",
    "print(f'P-value (left tail): {p_value_left}')\n",
    "print(f'P-value (two-tailed): {p_value_two_tailed}')\n",
    "print(f't-stat: {t_stat}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiomoth_test_20250225",
   "language": "python",
   "name": "test_audiomoth_env_20250225"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214.555px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
